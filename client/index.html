<!DOCTYPE html>
<html lang="cs">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ultra-Fast Voice AI - EniQ Assistant</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);
            min-height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            padding: 20px;
        }

        .container {
            background: rgba(255, 255, 255, 0.05);
            backdrop-filter: blur(10px);
            border-radius: 24px;
            padding: 40px;
            width: 100%;
            max-width: 600px;
            box-shadow: 0 25px 50px -12px rgba(0, 0, 0, 0.5);
            border: 1px solid rgba(255, 255, 255, 0.1);
        }

        h1 {
            color: #fff;
            text-align: center;
            margin-bottom: 10px;
            font-size: 28px;
            font-weight: 600;
        }

        .subtitle {
            color: rgba(255, 255, 255, 0.6);
            text-align: center;
            margin-bottom: 30px;
            font-size: 14px;
        }

        .status-bar {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 10px;
            margin-bottom: 30px;
            padding: 12px 20px;
            background: rgba(0, 0, 0, 0.3);
            border-radius: 12px;
        }

        .status-indicator {
            width: 12px;
            height: 12px;
            border-radius: 50%;
            background: #666;
            transition: all 0.3s ease;
        }

        .status-indicator.connected {
            background: #4ade80;
            box-shadow: 0 0 10px #4ade80;
        }

        .status-indicator.listening {
            background: #60a5fa;
            box-shadow: 0 0 10px #60a5fa;
            animation: pulse 1.5s infinite;
        }

        .status-indicator.speaking {
            background: #f472b6;
            box-shadow: 0 0 10px #f472b6;
            animation: pulse 0.8s infinite;
        }

        @keyframes pulse {
            0%, 100% { opacity: 1; transform: scale(1); }
            50% { opacity: 0.7; transform: scale(1.1); }
        }

        .status-text {
            color: rgba(255, 255, 255, 0.8);
            font-size: 14px;
        }

        .button-container {
            display: flex;
            justify-content: center;
            margin-bottom: 30px;
        }

        .start-button {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            padding: 18px 50px;
            font-size: 18px;
            font-weight: 600;
            border-radius: 50px;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 10px 30px rgba(102, 126, 234, 0.4);
        }

        .start-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 15px 40px rgba(102, 126, 234, 0.5);
        }

        .start-button:active {
            transform: translateY(0);
        }

        .start-button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
        }

        .start-button.active {
            background: linear-gradient(135deg, #ef4444 0%, #dc2626 100%);
            box-shadow: 0 10px 30px rgba(239, 68, 68, 0.4);
        }

        .transcript-container {
            background: rgba(0, 0, 0, 0.3);
            border-radius: 16px;
            padding: 20px;
            margin-bottom: 20px;
            max-height: 300px;
            overflow-y: auto;
        }

        .transcript-label {
            color: rgba(255, 255, 255, 0.5);
            font-size: 12px;
            text-transform: uppercase;
            letter-spacing: 1px;
            margin-bottom: 10px;
        }

        .transcript-content {
            color: rgba(255, 255, 255, 0.9);
            font-size: 16px;
            line-height: 1.6;
            min-height: 60px;
        }

        .transcript-content .user-text {
            color: #60a5fa;
        }

        .transcript-content .assistant-text {
            color: #4ade80;
        }

        .transcript-content .interim {
            opacity: 0.6;
            font-style: italic;
        }

        .message {
            margin-bottom: 12px;
            padding: 8px 12px;
            border-radius: 8px;
        }

        .message.user {
            background: rgba(96, 165, 250, 0.1);
            border-left: 3px solid #60a5fa;
        }

        .message.assistant {
            background: rgba(74, 222, 128, 0.1);
            border-left: 3px solid #4ade80;
        }

        .message-label {
            font-size: 11px;
            color: rgba(255, 255, 255, 0.4);
            margin-bottom: 4px;
        }

        .visualizer-container {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 4px;
            height: 60px;
            margin-bottom: 20px;
        }

        .visualizer-bar {
            width: 4px;
            background: linear-gradient(to top, #667eea, #764ba2);
            border-radius: 2px;
            transition: height 0.1s ease;
        }

        .debug-info {
            background: rgba(0, 0, 0, 0.3);
            border-radius: 12px;
            padding: 15px;
            font-family: monospace;
            font-size: 12px;
            color: rgba(255, 255, 255, 0.5);
            max-height: 150px;
            overflow-y: auto;
        }

        .debug-info .log-entry {
            margin-bottom: 4px;
        }

        .debug-info .log-entry.error {
            color: #ef4444;
        }

        .debug-info .log-entry.success {
            color: #4ade80;
        }

        /* CRITICAL: Big unmute overlay for iOS/Android autoplay policy */
        .unmute-overlay {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            z-index: 9999;
            animation: fadeIn 0.3s ease;
        }

        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        .unmute-overlay.hidden {
            display: none;
        }

        .unmute-button {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            padding: 40px 80px;
            font-size: 32px;
            font-weight: 700;
            border-radius: 20px;
            cursor: pointer;
            box-shadow: 0 20px 60px rgba(102, 126, 234, 0.6);
            transition: all 0.3s ease;
            text-transform: uppercase;
            letter-spacing: 2px;
        }

        .unmute-button:hover {
            transform: translateY(-5px);
            box-shadow: 0 25px 70px rgba(102, 126, 234, 0.7);
        }

        .unmute-button:active {
            transform: translateY(-2px);
        }

        .unmute-subtitle {
            color: rgba(255, 255, 255, 0.6);
            font-size: 18px;
            margin-top: 30px;
            text-align: center;
            max-width: 600px;
        }

        .unmute-icon {
            font-size: 80px;
            margin-bottom: 30px;
            animation: pulse 2s infinite;
        }
    </style>
</head>
<body>
    <!-- CRITICAL: Unmute overlay for iOS/Android autoplay -->
    <div class="unmute-overlay" id="unmuteOverlay">
        <div class="unmute-icon">üîä</div>
        <button class="unmute-button" id="unmuteButton">SPUSTI≈§ ASISTENTA</button>
        <p class="unmute-subtitle">Kliknut√≠m povol√≠te hlasov√© funkcie a pripoj√≠te sa k asistentovi</p>
    </div>

    <div class="container">
        <h1>‚ö° Ultra-Fast Voice AI</h1>
        <p class="subtitle">Google Speech V2 ‚Ä¢ Llama 3.3 70B ‚Ä¢ ElevenLabs Flash v2.5</p>

        <div class="status-bar">
            <div class="status-indicator" id="statusIndicator"></div>
            <span class="status-text" id="statusText">Odpojeno</span>
        </div>

        <div class="visualizer-container" id="visualizer">
            <!-- Audio visualizer bars will be created by JS -->
        </div>

        <div class="button-container">
            <button class="start-button" id="startButton">Spustit</button>
        </div>

        <div class="transcript-container">
            <div class="transcript-label">Konverzace</div>
            <div class="transcript-content" id="transcriptContent">
                <p style="color: rgba(255,255,255,0.4); font-style: italic;">Kliknƒõte na "Spustit" pro zah√°jen√≠ konverzace...</p>
            </div>
        </div>

        <div class="debug-info" id="debugInfo">
            <div class="log-entry">P≈ôipraveno k pou≈æit√≠</div>
        </div>
    </div>

    <script>
        // Configuration - Ultra-Fast Voice AI
        // Use wss:// for HTTPS (cloudflare), ws:// for HTTP (local)
        const WS_PROTOCOL = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
        const WS_URL = `${WS_PROTOCOL}//${window.location.host}/ws`;
        
        // Audio config for Google Cloud Speech V2 (Chirp 2)
        const SAMPLE_RATE = 16000;  // 16kHz LINEAR16 PCM required by Google
        const CHANNELS = 1;          // Mono audio

        // State
        let websocket = null;
        let mediaStream = null;
        let audioContext = null;
        let audioWorklet = null;
        let isActive = false;
        let currentUserText = '';
        let currentAssistantText = '';
        
        // ROBUST AUDIO PLAYBACK STATE
        let audioQueue = [];
        let isPlayingAudio = false;
        let assistantIsSpeaking = false;  // Track if assistant is speaking to pause mic
        let nextStartTime = 0;  // Scheduled start time for seamless playback
        let audioChunkCounter = 0;  // For debugging

        // DOM Elements
        const unmuteOverlay = document.getElementById('unmuteOverlay');
        const unmuteButton = document.getElementById('unmuteButton');
        const startButton = document.getElementById('startButton');
        const statusIndicator = document.getElementById('statusIndicator');
        const statusText = document.getElementById('statusText');
        const transcriptContent = document.getElementById('transcriptContent');
        const debugInfo = document.getElementById('debugInfo');
        const visualizer = document.getElementById('visualizer');

        // Initialize visualizer bars
        function initVisualizer() {
            visualizer.innerHTML = '';
            for (let i = 0; i < 32; i++) {
                const bar = document.createElement('div');
                bar.className = 'visualizer-bar';
                bar.style.height = '4px';
                visualizer.appendChild(bar);
            }
        }

        // Update visualizer with audio levels
        function updateVisualizer(levels) {
            const bars = visualizer.querySelectorAll('.visualizer-bar');
            bars.forEach((bar, i) => {
                const level = levels[i] || 0;
                bar.style.height = `${4 + level * 56}px`;
            });
        }

        // Reset visualizer
        function resetVisualizer() {
            const bars = visualizer.querySelectorAll('.visualizer-bar');
            bars.forEach(bar => {
                bar.style.height = '4px';
            });
        }

        // Logging
        function log(message, type = 'info') {
            const entry = document.createElement('div');
            entry.className = `log-entry ${type}`;
            entry.textContent = `[${new Date().toLocaleTimeString()}] ${message}`;
            debugInfo.insertBefore(entry, debugInfo.firstChild);
            
            // Keep only last 50 entries
            while (debugInfo.children.length > 50) {
                debugInfo.removeChild(debugInfo.lastChild);
            }
            
            console.log(`[${type.toUpperCase()}]`, message);
        }

        // Update status
        function setStatus(status, text) {
            statusIndicator.className = 'status-indicator ' + status;
            statusText.textContent = text;
        }

        // Add message to transcript
        function addMessage(text, type, isFinal = true) {
            if (type === 'user') {
                if (isFinal && text) {
                    const msg = document.createElement('div');
                    msg.className = 'message user';
                    msg.innerHTML = `<div class="message-label">Vy:</div>${text}`;
                    transcriptContent.appendChild(msg);
                    transcriptContent.scrollTop = transcriptContent.scrollHeight;
                    currentUserText = '';
                } else {
                    currentUserText = text;
                }
            } else if (type === 'assistant') {
                if (isFinal) {
                    if (currentAssistantText) {
                        const msg = document.createElement('div');
                        msg.className = 'message assistant';
                        msg.innerHTML = `<div class="message-label">Asistent:</div>${currentAssistantText}`;
                        transcriptContent.appendChild(msg);
                        transcriptContent.scrollTop = transcriptContent.scrollHeight;
                    }
                    currentAssistantText = '';
                } else {
                    currentAssistantText += text;
                }
            }
        }

        // Clear transcript
        function clearTranscript() {
            transcriptContent.innerHTML = '';
            currentUserText = '';
            currentAssistantText = '';
        }

        // ============================================================================
        // ROBUST AUDIO PLAYBACK SYSTEM - Using Web Audio API for seamless streaming
        // ============================================================================
        
        /**
         * Initialize AudioContext (must be called after user interaction)
         */
        async function initAudioPlayback() {
            if (!audioContext) {
                // Use default sample rate (browser will handle MP3 decoding)
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                console.log(`üéµ AudioContext created: state=${audioContext.state}, sampleRate=${audioContext.sampleRate}Hz`);
                log(`AudioContext created: state=${audioContext.state}, sampleRate=${audioContext.sampleRate}Hz`, 'success');
            }
            
            // CRITICAL: Resume AudioContext if suspended (requires user interaction)
            if (audioContext.state === 'suspended') {
                console.log('‚ö†Ô∏è AudioContext suspended, attempting to resume...');
                log('AudioContext suspended, attempting to resume...', 'info');
                try {
                    await audioContext.resume();
                    console.log(`‚úÖ AudioContext resumed: state=${audioContext.state}`);
                    log(`‚úÖ AudioContext resumed: state=${audioContext.state}`, 'success');
                } catch (error) {
                    console.error(`‚ùå Failed to resume AudioContext:`, error);
                    log(`‚ùå Failed to resume AudioContext: ${error.message}`, 'error');
                }
            }
            
            // Initialize nextStartTime to current time
            nextStartTime = audioContext.currentTime;
            
            // Double-check state
            if (audioContext.state !== 'running') {
                console.error(`‚ö†Ô∏è AudioContext not running (state=${audioContext.state})`);
                log(`‚ö†Ô∏è WARNING: AudioContext not running (state=${audioContext.state}). Audio may not play!`, 'error');
            } else {
                console.log('‚úÖ AudioContext is RUNNING and ready for playback');
            }
        }

        /**
         * Queue audio chunk for playback
         * CRITICAL: ElevenLabs sends PCM 24kHz 16-bit mono
         * We must convert Int16 ‚Üí Float32 for Web Audio API
         */
        async function queueAudioChunk(arrayBuffer) {
            if (!audioContext) {
                console.error('‚ùå AudioContext not initialized!');
                await initAudioPlayback();
            }
            
            audioChunkCounter++;
            const chunkId = audioChunkCounter;
            const size = arrayBuffer.byteLength;
            
            console.log(`üéµ [Chunk #${chunkId}] Received: ${size} bytes (PCM 24kHz 16-bit)`);
            log(`üì• Audio chunk #${chunkId}: ${size} bytes`, 'success');
            
            try {
                // PCM 24kHz 16-bit mono format from ElevenLabs
                const sampleRate = 24000;
                const numChannels = 1;
                
                // Convert raw PCM Int16 to Float32 array
                console.log(`üîÑ [Chunk #${chunkId}] Converting PCM Int16 ‚Üí Float32...`);
                const int16Array = new Int16Array(arrayBuffer);
                const float32Array = new Float32Array(int16Array.length);
                
                // CRITICAL: Convert Int16 (-32768 to 32767) to Float32 (-1.0 to 1.0)
                for (let i = 0; i < int16Array.length; i++) {
                    float32Array[i] = int16Array[i] / 32768.0;
                }
                
                console.log(`‚úÖ [Chunk #${chunkId}] Converted ${int16Array.length} samples`);
                
                // Create AudioBuffer manually
                const numSamples = float32Array.length;
                const duration = numSamples / sampleRate;
                
                const audioBuffer = audioContext.createBuffer(numChannels, numSamples, sampleRate);
                audioBuffer.getChannelData(0).set(float32Array);
                
                console.log(`‚úÖ [Chunk #${chunkId}] Created AudioBuffer: duration=${duration.toFixed(3)}s, samples=${numSamples}, rate=${sampleRate}Hz`);
                
                // Create audio source
                const source = audioContext.createBufferSource();
                source.buffer = audioBuffer;
                source.connect(audioContext.destination);
                
                // Calculate start time for seamless playback
                const now = audioContext.currentTime;
                const startTime = Math.max(now, nextStartTime);
                
                // Schedule playback
                source.start(startTime);
                console.log(`‚ñ∂Ô∏è [Chunk #${chunkId}] Scheduled at ${startTime.toFixed(3)}s (now=${now.toFixed(3)}s)`);
                
                // Update next start time for next chunk
                nextStartTime = startTime + duration;
                console.log(`‚è≠Ô∏è Next chunk will start at ${nextStartTime.toFixed(3)}s`);
                
                // Update UI
                setStatus('speaking', 'Asistent mluv√≠...');
                assistantIsSpeaking = true;
                
                // Handle playback end
                source.onended = () => {
                    console.log(`‚úì [Chunk #${chunkId}] Playback finished`);
                    
                    // Check if this was the last chunk
                    if (audioContext.currentTime >= nextStartTime - 0.1) {
                        console.log('üéµ All audio chunks played, returning to listening mode');
                        assistantIsSpeaking = false;
                        if (isActive) {
                            setStatus('listening', 'Poslouch√°m...');
                        }
                    }
                };
                
            } catch (error) {
                console.error(`‚ùå [Chunk #${chunkId}] Error:`, error);
                log(`‚ùå Error playing chunk #${chunkId}: ${error.message}`, 'error');
                log(`   AudioContext state: ${audioContext?.state}`, 'error');
            }
        }

        /**
         * Clear audio queue (for barge-in)
         */
        function clearAudioQueue() {
            // Reset scheduling
            if (audioContext) {
                nextStartTime = audioContext.currentTime;
            }
            assistantIsSpeaking = false;
            
            console.log('üßπ Audio queue cleared (barge-in)');
            log('Audio queue cleared (barge-in)', 'success');
            
            if (isActive) {
                setStatus('listening', 'Poslouch√°m...');
            }
        }

        // Initialize microphone
        async function initMicrophone() {
            try {
                mediaStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        sampleRate: SAMPLE_RATE,
                        channelCount: CHANNELS,
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true
                    }
                });
                
                log('Microphone access granted', 'success');
                return true;
                
            } catch (error) {
                log(`Microphone error: ${error.message}`, 'error');
                return false;
            }
        }

        // Process audio from microphone
        async function startAudioProcessing() {
            if (!mediaStream) return;

            const audioCtx = new (window.AudioContext || window.webkitAudioContext)({
                sampleRate: SAMPLE_RATE
            });

            const source = audioCtx.createMediaStreamSource(mediaStream);
            const analyser = audioCtx.createAnalyser();
            analyser.fftSize = 256;
            source.connect(analyser);

            // Use ScriptProcessor for audio capture (wider browser support)
            const processor = audioCtx.createScriptProcessor(4096, 1, 1);
            source.connect(processor);
            processor.connect(audioCtx.destination);

            const dataArray = new Uint8Array(analyser.frequencyBinCount);

            let audioChunkCount = 0;
            processor.onaudioprocess = (e) => {
                if (!isActive || !websocket || websocket.readyState !== WebSocket.OPEN) {
                    if (audioChunkCount > 0 && audioChunkCount % 100 === 0) {
                        log(`Audio not sent: active=${isActive}, ws=${websocket?.readyState}`, 'error');
                    }
                    return;
                }

                // Don't send audio while assistant is speaking (prevents echo/feedback)
                if (assistantIsSpeaking) {
                    return;
                }

                // Get audio data
                const inputData = e.inputBuffer.getChannelData(0);
                
                // Convert Float32 to Int16 PCM
                const pcmData = new Int16Array(inputData.length);
                for (let i = 0; i < inputData.length; i++) {
                    const s = Math.max(-1, Math.min(1, inputData[i]));
                    pcmData[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
                }

                // Send to WebSocket
                try {
                    websocket.send(pcmData.buffer);
                    audioChunkCount++;
                    // Log every 100 chunks (roughly every 25 seconds)
                    if (audioChunkCount % 100 === 0) {
                        log(`Sending audio... (${audioChunkCount} chunks sent)`);
                    }
                } catch (err) {
                    log(`Error sending audio: ${err.message}`, 'error');
                }

                // Update visualizer
                analyser.getByteFrequencyData(dataArray);
                const levels = Array.from(dataArray.slice(0, 32)).map(v => v / 255);
                updateVisualizer(levels);
            };

            audioWorklet = { processor, audioCtx, analyser };
            log('Audio processing started', 'success');
        }

        // Stop audio processing
        function stopAudioProcessing() {
            if (audioWorklet) {
                audioWorklet.processor.disconnect();
                audioWorklet.audioCtx.close();
                audioWorklet = null;
            }
            
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }
            
            resetVisualizer();
            log('Audio processing stopped');
        }

        // Connect WebSocket
        function connectWebSocket() {
            return new Promise((resolve, reject) => {
                log(`Connecting to ${WS_URL}...`);
                
                websocket = new WebSocket(WS_URL);
                
                websocket.onopen = () => {
                    log('WebSocket connected', 'success');
                    setStatus('connected', 'P≈ôipojeno');
                    resolve();
                };
                
                websocket.onclose = (event) => {
                    log(`WebSocket closed: code=${event.code}, reason="${event.reason}", clean=${event.wasClean}`, event.wasClean ? 'info' : 'error');
                    setStatus('', 'Odpojeno');
                    if (isActive) {
                        stop();
                    }
                };
                
                websocket.onerror = (error) => {
                    log('WebSocket error', 'error');
                    reject(error);
                };
                
                websocket.onmessage = async (event) => {
                    // CRITICAL: Detect binary vs text data
                    if (event.data instanceof Blob) {
                        // BINARY DATA - Audio chunk from ElevenLabs
                        console.log(`üîä Received audio chunk: ${event.data.size} bytes (Blob)`);
                        
                        // Convert Blob to ArrayBuffer
                        const arrayBuffer = await event.data.arrayBuffer();
                        console.log(`üîä Converted to ArrayBuffer: ${arrayBuffer.byteLength} bytes`);
                        
                        // Queue for playback
                        await queueAudioChunk(arrayBuffer);
                        
                    } else if (event.data instanceof ArrayBuffer) {
                        // BINARY DATA - ArrayBuffer directly
                        console.log(`üîä Received audio chunk: ${event.data.byteLength} bytes (ArrayBuffer)`);
                        await queueAudioChunk(event.data);
                        
                    } else if (typeof event.data === 'string') {
                        // TEXT DATA - JSON message
                        try {
                            const message = JSON.parse(event.data);
                            handleMessage(message);
                        } catch (e) {
                            console.error('‚ùå Invalid JSON:', event.data);
                            log(`Invalid JSON: ${event.data}`, 'error');
                        }
                    } else {
                        console.warn('‚ö†Ô∏è Unknown message type:', typeof event.data);
                    }
                };
            });
        }

        // Handle incoming messages
        function handleMessage(message) {
            switch (message.type) {
                case 'session_started':
                    log('Session started', 'success');
                    setStatus('listening', 'Poslouch√°m...');
                    break;
                    
                case 'user_text':
                    addMessage(message.text, 'user', message.is_final);
                    if (!message.is_final) {
                        log(`User (interim): ${message.text}`);
                    } else {
                        log(`User: ${message.text}`, 'success');
                    }
                    break;
                    
                case 'assistant_text':
                    addMessage(message.text, 'assistant', message.is_final);
                    if (!message.is_final && message.text) {
                        // Don't log every token, too noisy
                    } else if (message.is_final) {
                        log('Assistant response complete', 'success');
                    }
                    break;
                    
                case 'clear_audio':
                    log('Barge-in: Clearing audio', 'success');
                    clearAudioQueue();
                    setStatus('listening', 'Poslouch√°m...');
                    break;
                    
                case 'audio_end':
                    log('Audio stream ended');
                    break;
                    
                case 'listening':
                    log('Ready for input', 'success');
                    setStatus('listening', 'Poslouch√°m...');
                    break;
                
                case 'session_timeout':
                    log('Session timed out due to inactivity', 'error');
                    setStatus('', 'ƒåasov√Ω limit - odpojeno');
                    stop();
                    // Add timeout message to transcript
                    const timeoutMsg = document.createElement('div');
                    timeoutMsg.className = 'message';
                    timeoutMsg.style.color = 'rgba(255,150,150,0.8)';
                    timeoutMsg.style.fontStyle = 'italic';
                    timeoutMsg.innerHTML = '<div class="message-label">Syst√©m:</div>Relace byla ukonƒçena kv≈Øli neaktivitƒõ.';
                    transcriptContent.appendChild(timeoutMsg);
                    break;
                    
                case 'error':
                    log(`Server error: ${message.message}`, 'error');
                    break;
                    
                case 'pong':
                    // Heartbeat response from server
                    break;
                
                case 'ping':
                    // Server keepalive ping - respond with pong
                    if (websocket && websocket.readyState === WebSocket.OPEN) {
                        websocket.send(JSON.stringify({ type: 'pong' }));
                    }
                    break;
                    
                default:
                    log(`Unknown message type: ${message.type}`);
            }
        }

        // CRITICAL: First unmute - called when user clicks big unmute button
        async function unmute() {
            try {
                log('üîä Unmuting audio and initializing...', 'info');
                unmuteButton.disabled = true;
                unmuteButton.textContent = 'INICIALIZUJEM...';
                
                // CRITICAL: Initialize AudioContext FIRST (requires user interaction)
                await initAudioPlayback();
                
                // Verify AudioContext is running
                if (!audioContext || audioContext.state !== 'running') {
                    throw new Error(`AudioContext not running (state: ${audioContext?.state})`);
                }
                
                log('‚úÖ AudioContext ready, starting session...', 'success');
                
                // Hide unmute overlay
                unmuteOverlay.classList.add('hidden');
                
                // Auto-start the session
                await start();
                
            } catch (error) {
                log(`‚ùå Unmute error: ${error.message}`, 'error');
                unmuteButton.disabled = false;
                unmuteButton.textContent = 'SK√öSI≈§ ZNOVA';
            }
        }

        // Start session (called after unmute)
        async function start() {
            try {
                startButton.disabled = true;
                
                // AudioContext already initialized in unmute()
                log('AudioContext already initialized, connecting...', 'info');
                
                // Get microphone access
                if (!await initMicrophone()) {
                    throw new Error('Failed to access microphone');
                }
                
                // Connect WebSocket
                await connectWebSocket();
                
                // Start processing audio
                await startAudioProcessing();
                
                isActive = true;
                startButton.textContent = 'Zastavit';
                startButton.classList.add('active');
                startButton.disabled = false;
                
                // Clear old transcript
                clearTranscript();
                
                log('Voice session active', 'success');
                
            } catch (error) {
                log(`Start error: ${error.message}`, 'error');
                stop();
                startButton.disabled = false;
            }
        }

        // Stop session
        function stop() {
            isActive = false;
            
            // Send stop message
            if (websocket && websocket.readyState === WebSocket.OPEN) {
                websocket.send(JSON.stringify({ type: 'stop' }));
                websocket.close();
            }
            websocket = null;
            
            // Stop audio processing
            stopAudioProcessing();
            
            // Clear audio queue
            clearAudioQueue();
            
            // Reset UI
            startButton.textContent = 'Spustit';
            startButton.classList.remove('active');
            setStatus('', 'Odpojeno');
            
            log('Session stopped');
        }

        // Toggle session
        function toggle() {
            if (isActive) {
                stop();
            } else {
                start();
            }
        }

        // Heartbeat to keep connection alive (reduced to 20s to match server)
        setInterval(() => {
            if (websocket && websocket.readyState === WebSocket.OPEN) {
                websocket.send(JSON.stringify({ type: 'ping' }));
            }
        }, 20000);

        // Initialize
        document.addEventListener('DOMContentLoaded', () => {
            initVisualizer();
            
            // CRITICAL: Unmute button handler - first user interaction
            unmuteButton.addEventListener('click', unmute);
            
            // Regular start/stop button
            startButton.addEventListener('click', toggle);
            
            // Log environment info
            log(`Protocol: ${window.location.protocol}`);
            log(`Host: ${window.location.host}`);
            log(`Secure context: ${window.isSecureContext}`);
            
            // Check for required APIs
            if (!window.isSecureContext) {
                log('ERROR: Str√°nka nie je v secure context!', 'error');
                log('Mikrof√≥n vy≈æaduje HTTPS. Pou≈æi https:// URL', 'error');
                unmuteButton.disabled = true;
                unmuteButton.textContent = 'HTTPS REQUIRED';
            } else if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                log('getUserMedia nie je dostupn√©', 'error');
                log('Sk√∫s obnovi≈• str√°nku alebo povoli≈• mikrof√≥n v nastaveniach', 'error');
                unmuteButton.disabled = true;
                unmuteButton.textContent = 'MIKROF√ìN NEDOSTUPN√ù';
            }
            
            if (!window.WebSocket) {
                log('WebSocket nie je podporovan√Ω', 'error');
                unmuteButton.disabled = true;
                unmuteButton.textContent = 'WEBSOCKET NEPODPOROVAN√ù';
            }
            
            if (!unmuteButton.disabled) {
                log('‚úÖ Klient pripraven√Ω - klikni SPUSTI≈§ ASISTENTA', 'success');
            }
        });
    </script>
</body>
</html>
